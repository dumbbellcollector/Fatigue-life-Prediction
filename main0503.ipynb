{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 라이브러리 및 설정\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.impute import SimpleImputer # 필요시 기본 결측치 처리 (이제 사용 안 함)\n",
    "# from sklearn.ensemble import RandomForestRegressor # HB 결측치 처리용 (이제 사용 안 함)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib # 스케일러 저장용\n",
    "import streamlit as st # GUI용 (Cell 12에서만 사용)\n",
    "import os # GUI용 파일 경로 확인\n",
    "\n",
    "# 데이터 경로 정의 (파일명 확인 및 수정)\n",
    "data_path = \"/Users/yeojoon/Documents/PyWorkspace_CreativeDesign/TrainSet0507.xlsx\" #BakUp0413Materiadata.xlsx\"\n",
    "\n",
    "# 장치 설정 (가능하면 GPU/MPS, 아니면 CPU 사용)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"사용 장치: {device}\")\n",
    "\n",
    "# 재현성을 위한 랜덤 시드 설정\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "elif device == 'mps':\n",
    "     pass\n",
    "\n",
    "print(\"라이브러리 임포트 및 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 데이터 로딩 및 초기 검사\n",
    "\n",
    "try:\n",
    "    # 'Tensile' 시트 로드\n",
    "    df_tensile = pd.read_excel(data_path, sheet_name='Tensile', header=0)\n",
    "    print(\"--- Tensile 시트 ---\")\n",
    "    print(f\"형태: {df_tensile.shape}\")\n",
    "    print(\"\\n컬럼:\", df_tensile.columns.tolist())\n",
    "    print(\"\\n처음 5행:\\n\", df_tensile.head())\n",
    "    print(\"\\n데이터 타입:\\n\", df_tensile.dtypes)\n",
    "    print(\"\\n결측치 요약:\\n\", df_tensile.isnull().sum())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: {data_path}에서 파일을 찾을 수 없습니다\")\n",
    "except Exception as e:\n",
    "    print(f\"데이터 로딩 중 오류 발생: {e}\")\n",
    "\n",
    "# 필요한 컬럼명 확인 (주석)\n",
    "# E: 'E'\n",
    "# YS: 'YS'\n",
    "# TS: 'TS'\n",
    "# HB: 'HB'\n",
    "# HV: 'HV'\n",
    "# sf: 'sf' (sigma_f')\n",
    "# b: 'b'\n",
    "# ef: 'ef' (epsilon_f')\n",
    "# c: 'c'\n",
    "\n",
    "\n",
    "# np_col = \"n'\" # 선택사항\n",
    "# kp_col = \"K' (MPa)\" # 선택사항\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 컬럼 이름 변경 및 타입 변환\n",
    "\n",
    "# --- 컬럼 이름 매핑 정의 (실제 파일 컬럼명 기준) ---\n",
    "column_mapping = {\n",
    "    'E': 'E_GPa',\n",
    "    'YS': 'YS_MPa',\n",
    "    'TS': 'TS_MPa',\n",
    "    'HB': 'HB',\n",
    "    'HV': 'HV',\n",
    "    \"sf\": 'spf_MPa',\n",
    "    'b': 'b',\n",
    "    \"ef\": 'epf',\n",
    "    'c': 'c',\n",
    "    # 사용할 경우 선택적 컬럼 추가\n",
    "    # \"n'\": 'np',\n",
    "    # \"K' (MPa)\": 'Kp_MPa',\n",
    "    # 'Material Name', 'RA (%)' 등과 같은 다른 관련 컬럼 추가\n",
    "    #'RA': 'RA_percent' # RA가 존재하고 필요한 경우 예시\n",
    "}\n",
    "\n",
    "# 사용할 컬럼만 선택 및 이름 변경\n",
    "required_cols = list(column_mapping.keys())\n",
    "available_cols = [col for col in required_cols if col in df_tensile.columns]\n",
    "missing_cols = [col for col in required_cols if col not in available_cols]\n",
    "\n",
    "print(f\"\\n사용 가능한 컬럼: {available_cols}\")\n",
    "if missing_cols:\n",
    "    print(f\"경고: 다음 컬럼이 데이터에 없습니다: {missing_cols}. 해당 컬럼 없이 진행합니다.\")\n",
    "    # 필수 컬럼 누락 시 에러 처리 또는 중지 로직 추가 가능\n",
    "\n",
    "# 사용 가능한 컬럼만으로 매핑 재생성\n",
    "column_mapping_available = {k: v for k, v in column_mapping.items() if k in available_cols}\n",
    "\n",
    "df = df_tensile[available_cols].copy()\n",
    "df.rename(columns=column_mapping_available, inplace=True)\n",
    "\n",
    "hv_col_exists = 'HV' in df.columns\n",
    "\n",
    "# 숫자형 변환 (오류는 NaN)\n",
    "numeric_cols_base = ['E_GPa', 'YS_MPa', 'TS_MPa', 'HB', 'spf_MPa', 'b', 'epf', 'c']\n",
    "numeric_cols = [col for col in numeric_cols_base if col in df.columns] # 존재하는 컬럼만 대상으로 함\n",
    "if hv_col_exists:\n",
    "    numeric_cols.append('HV')\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print(\"\\n이름 변경 및 숫자 변환 후 DataFrame:\")\n",
    "print(df.head())\n",
    "print(\"\\n데이터 타입:\\n\", df.dtypes)\n",
    "print(\"\\n강제 변환 후 결측치 요약:\\n\", df.isnull().sum())\n",
    "\n",
    "# E 단위 변환 (GPa -> MPa)\n",
    "if 'E_GPa' in df.columns:\n",
    "    df['E_MPa'] = df['E_GPa'] * 1000\n",
    "    df.drop(columns=['E_GPa'], inplace=True)\n",
    "    print(\"\\nE를 MPa로 변환했습니다('E_MPa' 컬럼 생성됨).\")\n",
    "    # Ensure E_MPa is in numeric_cols if E_GPa was\n",
    "    if 'E_GPa' in numeric_cols:\n",
    "        numeric_cols.remove('E_GPa')\n",
    "        if 'E_MPa' not in numeric_cols:\n",
    "             numeric_cols.append('E_MPa')\n",
    "\n",
    "\n",
    "# 최종 특성 및 타겟 컬럼 이름 정의 (HB는 Cell 4에서 처리 후 추가)\n",
    "feature_cols_base = ['E_MPa', 'YS_MPa', 'TS_MPa']\n",
    "feature_cols = [col for col in feature_cols_base if col in df.columns] # 존재하는 특성만 사용\n",
    "target_cols_base = ['spf_MPa', 'b', 'epf', 'c']\n",
    "target_cols = [col for col in target_cols_base if col in df.columns] # 존재하는 타겟만 사용\n",
    "\n",
    "# 타겟 변수 누락 행 제거\n",
    "if not target_cols:\n",
    "     print(\"오류: 타겟 변수가 하나도 없습니다. 학습을 진행할 수 없습니다.\")\n",
    "else:\n",
    "    initial_rows = len(df)\n",
    "    df.dropna(subset=target_cols, inplace=True)\n",
    "    print(f\"\\n타겟 값({target_cols})이 누락된 {initial_rows - len(df)}개 행을 삭제했습니다.\")\n",
    "    print(f\"남은 행: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: HB/HV 처리 (수정됨 - ML/Median 대체 제거, HV 변환 우선)\n",
    "\n",
    "hb_col_name = 'HB' # 원본 HB 컬럼명 (매핑 후 이름)\n",
    "hv_col_name = 'HV' # 원본 HV 컬럼명 (매핑 후 이름)\n",
    "hb_processed_col = 'HB_processed' # 새로 생성할 처리된 HB 컬럼\n",
    "\n",
    "if hb_col_name not in df.columns:\n",
    "    print(f\"경고: HB 컬럼('{hb_col_name}')이 없습니다. HB 특성을 사용할 수 없습니다.\")\n",
    "    # HB 컬럼이 아예 없으면 hb_processed 생성 불가 -> 이후 로직에서 HB 사용 제외 필요\n",
    "    hb_available = False\n",
    "else:\n",
    "    hb_available = True\n",
    "    df[hb_processed_col] = df[hb_col_name].copy() # 기본값: 원본 HB 사용\n",
    "\n",
    "    # HB가 NaN 이고 HV가 있는 경우, HV -> HB 변환\n",
    "    if hv_col_exists:\n",
    "        conversion_mask = df[hb_processed_col].isnull() & df[hv_col_name].notnull()\n",
    "        # 예시 변환 (필요시 수정)\n",
    "        df.loc[conversion_mask, hb_processed_col] = 0.95 * df.loc[conversion_mask, hv_col_name]\n",
    "        print(f\"Imputed {conversion_mask.sum()} missing HB values using HV conversion.\")\n",
    "    else:\n",
    "        print(\"HV column not found. Skipping HV->HB conversion.\")\n",
    "\n",
    "    # HB_processed 컬럼을 feature_cols 에 추가\n",
    "    feature_cols.append(hb_processed_col)\n",
    "    print(f\"\\n'{hb_processed_col}' 컬럼을 특성에 추가했습니다.\")\n",
    "\n",
    "    # HB_processed 가 여전히 NaN 인 행은 HB/HV 모두 없는 경우 -> 분석에서 제외\n",
    "    initial_rows_before_hb_drop = len(df)\n",
    "    df.dropna(subset=[hb_processed_col], inplace=True)\n",
    "    dropped_for_hb = initial_rows_before_hb_drop - len(df)\n",
    "    if dropped_for_hb > 0:\n",
    "        print(f\"\\nDropped {dropped_for_hb} rows where both HB and HV were missing or invalid.\")\n",
    "\n",
    "\n",
    "# 최종 특성 컬럼에서 결측치가 있는 행 제거 (E, YS, TS 등 포함)\n",
    "initial_rows_before_feature_drop = len(df)\n",
    "df.dropna(subset=feature_cols, inplace=True)\n",
    "dropped_for_features = initial_rows_before_feature_drop - len(df)\n",
    "if dropped_for_features > 0:\n",
    "     print(f\"Dropped {dropped_for_features} rows with missing values in features: {feature_cols}.\")\n",
    "\n",
    "print(f\"\\nFinal Feature Columns: {feature_cols}\")\n",
    "print(f\"Target Columns: {target_cols}\")\n",
    "print(\"\\nDataFrame after HB/HV processing and final NA drop:\")\n",
    "print(df[feature_cols + target_cols].head())\n",
    "print(\"\\nMissing values check for final features and targets:\")\n",
    "print(df[feature_cols + target_cols].isnull().sum())\n",
    "print(f\"Final number of samples for training/testing: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 탐색적 데이터 분석 (EDA) - 큰 변경 없음, HB_imputed_flag_col 관련 부분 제거\n",
    "\n",
    "print(\"\\n--- Exploratory Data Analysis ---\")\n",
    "\n",
    "if len(df) > 0: # 데이터가 남아있는 경우에만 EDA 수행\n",
    "    # 1. Descriptive Statistics\n",
    "    print(\"\\nDescriptive Statistics (Features and Targets):\")\n",
    "    print(df[feature_cols + target_cols].describe())\n",
    "\n",
    "    # 2. Distributions\n",
    "    print(\"\\nPlotting Distributions...\")\n",
    "    n_features = len(feature_cols)\n",
    "    n_targets = len(target_cols)\n",
    "    # figure 크기 동적 조절\n",
    "    fig_height = max(4 * (n_features + n_targets), 12)\n",
    "    fig, axes = plt.subplots(n_features + n_targets, 2, figsize=(12, fig_height), facecolor='none')\n",
    "\n",
    "    if n_features + n_targets == 1: # 축 객체가 1차원 배열인 경우 처리\n",
    "        axes = np.array([axes])\n",
    "        \n",
    "    # custom color 정의\n",
    "    custom_color = (82/255, 151/255, 106/255)  # RGB 값\n",
    "\n",
    "    for i, col in enumerate(feature_cols + target_cols):\n",
    "        if col in df.columns: # 컬럼 존재 확인\n",
    "             # 1열(히스토그램)에 custom color 적용\n",
    "             sns.histplot(df[col], kde=True, ax=axes[i, 0], color=custom_color)\n",
    "             axes[i, 0].set_title(f'Histogram of {col}', color=custom_color)\n",
    "             axes[i, 0].set_xlabel(col, color=custom_color)\n",
    "             axes[i, 0].set_ylabel('Frequency', color=custom_color)\n",
    "             axes[i, 0].tick_params(axis='x', colors=custom_color)\n",
    "             axes[i, 0].tick_params(axis='y', colors=custom_color)\n",
    "             axes[i, 0].spines['bottom'].set_color(custom_color)\n",
    "             axes[i, 0].spines['top'].set_color(custom_color)\n",
    "             axes[i, 0].spines['left'].set_color(custom_color)\n",
    "             axes[i, 0].spines['right'].set_color(custom_color)\n",
    "             axes[i, 0].set_facecolor('none')  # 배경 투명하게 설정\n",
    "             \n",
    "             # 2열(박스플롯)은 기본 색상 유지\n",
    "             sns.boxplot(x=df[col], ax=axes[i, 1])\n",
    "             axes[i, 1].set_title(f'Boxplot of {col}')\n",
    "             axes[i, 1].set_facecolor('none')  # 배경 투명하게 설정\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # custom color 그래프 추가 (첫 번째 컬럼만)\n",
    "    if len(feature_cols) > 0:\n",
    "        custom_color = (82/255, 151/255, 106/255)  # RGB 값\n",
    "        first_col = feature_cols[0]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6), facecolor='none')  # 배경 투명하게 설정\n",
    "        sns.histplot(df[first_col], kde=True, ax=ax, color=custom_color)\n",
    "        \n",
    "        # 모든 요소를 custom color로 설정\n",
    "        ax.set_title(f'Histogram of {first_col}', color=custom_color)\n",
    "        ax.set_xlabel(first_col, color=custom_color)\n",
    "        ax.set_ylabel('Frequency', color=custom_color)\n",
    "        ax.tick_params(axis='x', colors=custom_color)\n",
    "        ax.tick_params(axis='y', colors=custom_color)\n",
    "        ax.spines['bottom'].set_color(custom_color)\n",
    "        ax.spines['top'].set_color(custom_color)\n",
    "        ax.spines['left'].set_color(custom_color)\n",
    "        ax.spines['right'].set_color(custom_color)\n",
    "        ax.set_facecolor('none')  # 배경 투명하게 설정\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 3. Correlation Matrix\n",
    "    print(\"\\nCalculating and Plotting Correlation Matrix...\")\n",
    "    # 상관관계 계산 전, 수치형 데이터만 있는지 확인\n",
    "    numeric_df_for_corr = df[feature_cols + target_cols].select_dtypes(include=np.number)\n",
    "    if not numeric_df_for_corr.empty:\n",
    "        correlation_matrix = numeric_df_for_corr.corr()\n",
    "        plt.figure(figsize=(10, 8), facecolor='none')  # 배경 투명하게 설정\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "        plt.title('Correlation Matrix of Features and Targets')\n",
    "        plt.gca().set_facecolor('none')  # 히트맵 배경 투명하게 설정\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No numeric columns found for correlation analysis.\")\n",
    "\n",
    "\n",
    "    # 4. Pair Plots (optional)\n",
    "    # ...\n",
    "\n",
    "    # 5. Analysis of Target Parameters (Physical Plausibility)\n",
    "    print(\"\\nAnalysis of target parameters:\")\n",
    "    if 'b' in df.columns: print(\"Range of 'b':\", df['b'].min(), df['b'].max())\n",
    "    if 'c' in df.columns: print(\"Range of 'c':\", df['c'].min(), df['c'].max())\n",
    "    if 'b' in df.columns: print(f\"Percentage of non-negative 'b': { (df['b'] >= 0).mean() * 100:.2f}%\")\n",
    "    if 'c' in df.columns: print(f\"Percentage of non-negative 'c': { (df['c'] >= 0).mean() * 100:.2f}%\")\n",
    "    if 'spf_MPa' in df.columns and 'TS_MPa' in df.columns:\n",
    "         print(f\"Percentage where spf_MPa <= TS_MPa: { (df['spf_MPa'] <= df['TS_MPa']).mean() * 100:.2f}%\")\n",
    "\n",
    "    # HB imputation status 비교 제거 (HB_imputed_flag_col 없음)\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo data available for EDA after preprocessing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 데이터 준비 (PyTorch용) - df 사용 확인\n",
    "\n",
    "print(\"\\n--- Preparing Data for PyTorch (Tensile Model) ---\")\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Select final features (X) and targets (y) for tensile model\n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_cols].values\n",
    "    # E_MPa 컬럼이 있는지 확인 후 추출\n",
    "    if 'E_MPa' in df.columns:\n",
    "        E_for_loss = df['E_MPa'].values\n",
    "        print(f\"E for loss shape: {E_for_loss.shape}\")\n",
    "    else:\n",
    "        print(\"Warning: E_MPa column not found. Cannot extract E for physics loss calculation.\")\n",
    "        E_for_loss = np.zeros(len(df)) # 임시값 또는 다른 처리 필요\n",
    "\n",
    "    print(f\"Features shape (X): {X.shape}\")\n",
    "    print(f\"Targets shape (y): {y.shape}\")\n",
    "\n",
    "    # 1. Train-Validation-Test Split\n",
    "    X_train_val, X_test, y_train_val, y_test, E_train_val, E_test = train_test_split(\n",
    "        X, y, E_for_loss, test_size=0.15, random_state=seed)\n",
    "    X_train, X_val, y_train, y_val, E_train, E_val = train_test_split(\n",
    "        X_train_val, y_train_val, E_train_val, test_size=0.1765, random_state=seed) # 0.1765 * 0.85 ≈ 0.15\n",
    "\n",
    "    print(f\"Train set size: {X_train.shape[0]}\")\n",
    "    print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "    print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "    # 2. Scaling\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_val_scaled = scaler_X.transform(X_val)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_val_scaled = scaler_y.transform(y_val)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "    joblib.dump(scaler_X, 'scaler_X.pkl')\n",
    "    joblib.dump(scaler_y, 'scaler_y.pkl')\n",
    "    print(\"Features and targets scaled. Scalers saved.\")\n",
    "\n",
    "    # 3. Create PyTorch Datasets and DataLoaders\n",
    "    batch_size = 16\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "    E_train_tensor = torch.tensor(E_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
    "    E_val_tensor = torch.tensor(E_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "    E_test_tensor = torch.tensor(E_test, dtype=torch.float32).unsqueeze(1)\n",
    "    y_test_orig_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor, E_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor, E_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor, E_test_tensor, y_test_orig_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"PyTorch Datasets and DataLoaders created for tensile model.\")\n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "    print(f\"Number of test batches: {len(test_loader)}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\\\nNo data available for PyTorch preparation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: PINN 모델 정의\n",
    "\n",
    "if len(df) > 0:\n",
    "    class FatiguePINN(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, hidden_dims=[128, 256, 128]):\n",
    "            super().__init__()\n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "\n",
    "            layers = []\n",
    "            last_dim = input_dim\n",
    "            for hidden_dim in hidden_dims:\n",
    "                layers.append(nn.Linear(last_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                last_dim = hidden_dim\n",
    "            layers.append(nn.Linear(last_dim, output_dim))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "\n",
    "    # 모델 인스턴스화\n",
    "    input_dim = X_train_scaled.shape[1] if 'X_train_scaled' in locals() else 4 # 예비 차원\n",
    "    output_dim = y_train_scaled.shape[1] if 'y_train_scaled' in locals() else 4 # 예비 차원\n",
    "    hidden_dims = [128, 256, 128]\n",
    "    model = FatiguePINN(input_dim, output_dim, hidden_dims).to(device)\n",
    "\n",
    "    print(\"PINN Model Definition (FatiguePINN):\")\n",
    "    print(model)\n",
    "\n",
    "    # 더미 테스트\n",
    "    try:\n",
    "        dummy_input = torch.randn(batch_size, input_dim).to(device)\n",
    "        dummy_output = model(dummy_input)\n",
    "        print(f\"\\\\nDummy input shape: {dummy_input.shape}\")\n",
    "        print(f\"Dummy output shape: {dummy_output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\nCould not perform dummy test: {e}\")\n",
    "else:\n",
    "    print(\"\\\\nSkipping model definition as no data is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: 손실 함수 정의 (PINN) - 변경 없음\n",
    "\n",
    "if len(df) > 0:\n",
    "    def pinn_loss_function(model_outputs_scaled, targets_scaled, E_unscaled, scaler_y, lambda_physics=0.1, num_physics_points=20, Nf_min=1e1, Nf_max=1e7, device='cpu'):\n",
    "        \"\"\"\n",
    "        Calculates the combined Data and Physics loss for the Fatigue PINN.\n",
    "        (Function definition remains the same as provided previously)\n",
    "        \"\"\"\n",
    "        # 1. Data Loss (MSE on scaled parameters)\n",
    "        data_loss_fn = nn.MSELoss()\n",
    "        data_loss = data_loss_fn(model_outputs_scaled, targets_scaled)\n",
    "\n",
    "        # 2. Physics Loss (Coffin-Manson)\n",
    "        physics_loss = torch.tensor(0.0, device=device) # 기본값 초기화\n",
    "        try:\n",
    "            # Inverse transform the *predicted* parameters\n",
    "            model_outputs_unscaled_np = scaler_y.inverse_transform(model_outputs_scaled.detach().cpu().numpy())\n",
    "            model_outputs_unscaled = torch.tensor(model_outputs_unscaled_np, dtype=torch.float32).to(device)\n",
    "\n",
    "            spf_pred = model_outputs_unscaled[:, 0].unsqueeze(1)\n",
    "            b_pred = model_outputs_unscaled[:, 1].unsqueeze(1)\n",
    "            epf_pred = model_outputs_unscaled[:, 2].unsqueeze(1)\n",
    "            c_pred = model_outputs_unscaled[:, 3].unsqueeze(1)\n",
    "\n",
    "            reversals = torch.logspace(torch.log10(torch.tensor(Nf_min)), torch.log10(torch.tensor(Nf_max)), num_physics_points).unsqueeze(0).to(device)\n",
    "\n",
    "            # Avoid division by zero or invalid E\n",
    "            E_unscaled_safe = torch.where(E_unscaled <= 0, torch.tensor(1e-6, device=device), E_unscaled)\n",
    "\n",
    "            epsilon_a_pred = (spf_pred / E_unscaled_safe) * (reversals ** b_pred) + epf_pred * (reversals ** c_pred)\n",
    "\n",
    "            # Inverse transform the *target* parameters\n",
    "            targets_unscaled_np = scaler_y.inverse_transform(targets_scaled.detach().cpu().numpy())\n",
    "            targets_unscaled = torch.tensor(targets_unscaled_np, dtype=torch.float32).to(device)\n",
    "            spf_true = targets_unscaled[:, 0].unsqueeze(1)\n",
    "            b_true = targets_unscaled[:, 1].unsqueeze(1)\n",
    "            epf_true = targets_unscaled[:, 2].unsqueeze(1)\n",
    "            c_true = targets_unscaled[:, 3].unsqueeze(1)\n",
    "            epsilon_a_true = (spf_true / E_unscaled_safe) * (reversals ** b_true) + epf_true * (reversals ** c_true)\n",
    "\n",
    "            physics_loss = torch.mean((epsilon_a_pred - epsilon_a_true)**2)\n",
    "\n",
    "            # Handle potential NaN in physics loss (e.g., from large exponents)\n",
    "            if torch.isnan(physics_loss):\n",
    "                 physics_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error calculating physics loss: {e}. Setting physics loss to 0 for this batch.\")\n",
    "            physics_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "\n",
    "        # 3. Total Loss\n",
    "        total_loss = data_loss + lambda_physics * physics_loss\n",
    "\n",
    "        return total_loss, data_loss, physics_loss\n",
    "\n",
    "    # --- Test the loss function ---\n",
    "    try:\n",
    "        sample_X, sample_y_scaled, sample_E = next(iter(train_loader))\n",
    "        sample_X, sample_y_scaled, sample_E = sample_X.to(device), sample_y_scaled.to(device), sample_E.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "             sample_output_scaled = model(sample_X)\n",
    "\n",
    "        lambda_physics_value = 0.01 # 테스트 및 튜닝용 람다 값\n",
    "        total_loss, data_loss, physics_loss = pinn_loss_function(\n",
    "            sample_output_scaled, sample_y_scaled, sample_E, scaler_y, lambda_physics=lambda_physics_value, device=device\n",
    "        )\n",
    "\n",
    "        print(f\"\\nTesting Loss Function:\")\n",
    "        print(f\"  Sample Output (scaled) shape: {sample_output_scaled.shape}\")\n",
    "        print(f\"  Sample Target (scaled) shape: {sample_y_scaled.shape}\")\n",
    "        print(f\"  Sample E (unscaled) shape: {sample_E.shape}\")\n",
    "        print(f\"  Calculated Total Loss: {total_loss.item():.4f}\")\n",
    "        print(f\"  Calculated Data Loss: {data_loss.item():.4f}\")\n",
    "        print(f\"  Calculated Physics Loss: {physics_loss.item():.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCould not test loss function, likely due to data loader issue: {e}\")\n",
    "        lambda_physics_value = 0.01 # 기본값 설정\n",
    "\n",
    "else:\n",
    "    print(\"\\\\nSkipping loss function definition and testing.\")\n",
    "    lambda_physics_value = 0.01 # 기본값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: 학습 루프 (Training Loop)\n",
    "\n",
    "if len(df) > 0 and 'train_loader' in locals() and 'val_loader' in locals(): # 데이터 로더 존재 확인\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    epochs = 200\n",
    "    patience = 20\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Lists to store loss history\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_data_loss_hist = []\n",
    "    train_phys_loss_hist = []\n",
    "    val_data_loss_hist = []\n",
    "    val_phys_loss_hist = []\n",
    "\n",
    "    print(f\"\\n--- Starting Training ---\")\n",
    "    print(f\"Epochs: {epochs}, LR: {learning_rate}, Lambda_Physics: {lambda_physics_value}, Patience: {patience}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_epoch_loss = 0.0\n",
    "        train_epoch_data_loss = 0.0\n",
    "        train_epoch_phys_loss = 0.0\n",
    "\n",
    "        for batch_X, batch_y_scaled, batch_E in train_loader:\n",
    "            batch_X, batch_y_scaled, batch_E = batch_X.to(device), batch_y_scaled.to(device), batch_E.to(device)\n",
    "\n",
    "            outputs_scaled = model(batch_X)\n",
    "            loss, data_loss, phys_loss = pinn_loss_function(\n",
    "                outputs_scaled, batch_y_scaled, batch_E, scaler_y, lambda_physics=lambda_physics_value, device=device\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_epoch_loss += loss.item()\n",
    "            train_epoch_data_loss += data_loss.item()\n",
    "            # 물리 손실이 0이 아닐 때만 기록 (NaN 방지)\n",
    "            train_epoch_phys_loss += phys_loss.item() if not torch.isnan(phys_loss) else 0.0\n",
    "\n",
    "\n",
    "        avg_train_loss = train_epoch_loss / len(train_loader)\n",
    "        avg_train_data_loss = train_epoch_data_loss / len(train_loader)\n",
    "        avg_train_phys_loss = train_epoch_phys_loss / len(train_loader)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        train_data_loss_hist.append(avg_train_data_loss)\n",
    "        train_phys_loss_hist.append(avg_train_phys_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0.0\n",
    "        val_epoch_data_loss = 0.0\n",
    "        val_epoch_phys_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X_val, batch_y_val_scaled, batch_E_val in val_loader:\n",
    "                batch_X_val, batch_y_val_scaled, batch_E_val = batch_X_val.to(device), batch_y_val_scaled.to(device), batch_E_val.to(device)\n",
    "\n",
    "                outputs_val_scaled = model(batch_X_val)\n",
    "                val_loss, val_data_loss, val_phys_loss = pinn_loss_function(\n",
    "                     outputs_val_scaled, batch_y_val_scaled, batch_E_val, scaler_y, lambda_physics=lambda_physics_value, device=device\n",
    "                )\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_data_loss += val_data_loss.item()\n",
    "                val_epoch_phys_loss += val_phys_loss.item() if not torch.isnan(val_phys_loss) else 0.0\n",
    "\n",
    "\n",
    "        avg_val_loss = val_epoch_loss / len(val_loader)\n",
    "        avg_val_data_loss = val_epoch_data_loss / len(val_loader)\n",
    "        avg_val_phys_loss = val_epoch_phys_loss / len(val_loader)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        val_data_loss_hist.append(avg_val_data_loss)\n",
    "        val_phys_loss_hist.append(avg_val_phys_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} (Data: {avg_train_data_loss:.4f}, Phys: {avg_train_phys_loss:.4f}), \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} (Data: {avg_val_data_loss:.4f}, Phys: {avg_val_phys_loss:.4f})\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_fatigue_pinn_model.pth')\n",
    "            print(f\"  Validation loss improved. Saved best model.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "    # Plot loss history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss_history, label='Total Train Loss')\n",
    "    plt.plot(val_loss_history, label='Total Validation Loss')\n",
    "    plt.title('Total Loss History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_data_loss_hist, label='Train Data Loss')\n",
    "    plt.plot(val_data_loss_hist, label='Val Data Loss')\n",
    "    # 물리 손실 시각화 시 lambda 곱해서 스케일 확인\n",
    "    plt.plot(np.array(train_phys_loss_hist) * lambda_physics_value, label=f'Train Physics Loss (x{lambda_physics_value:.2f})')\n",
    "    plt.plot(np.array(val_phys_loss_hist) * lambda_physics_value, label=f'Val Physics Loss (x{lambda_physics_value:.2f})')\n",
    "    plt.title('Data vs Physics Loss History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss Component')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\\\nSkipping training loop as data is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Evaluation\n",
    "\n",
    "# Load the best model saved during training\n",
    "model.load_state_dict(torch.load('best_fatigue_pinn_model.pth', map_location=device))\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "all_preds_scaled = []\n",
    "all_targets_scaled = []\n",
    "all_targets_orig = []\n",
    "\n",
    "print(\"\\n--- 테스트 세트 평가 중 ---\")\n",
    "with torch.no_grad():\n",
    "    for batch_X_test, batch_y_test_scaled, batch_E_test, batch_y_test_orig in test_loader:\n",
    "        batch_X_test, batch_y_test_scaled = batch_X_test.to(device), batch_y_test_scaled.to(device)\n",
    "\n",
    "        # Predict scaled parameters\n",
    "        outputs_test_scaled = model(batch_X_test)\n",
    "\n",
    "        all_preds_scaled.append(outputs_test_scaled.cpu().numpy())\n",
    "        all_targets_scaled.append(batch_y_test_scaled.cpu().numpy())\n",
    "        all_targets_orig.append(batch_y_test_orig.cpu().numpy())\n",
    "\n",
    "# Concatenate results from all batches\n",
    "all_preds_scaled = np.concatenate(all_preds_scaled, axis=0)\n",
    "all_targets_scaled = np.concatenate(all_targets_scaled, axis=0)\n",
    "all_targets_orig = np.concatenate(all_targets_orig, axis=0)\n",
    "\n",
    "# Inverse transform predictions to original scale\n",
    "all_preds_orig = scaler_y.inverse_transform(all_preds_scaled)\n",
    "\n",
    "# Calculate metrics for each parameter in original scale\n",
    "metrics = {}\n",
    "print(\"\\n테스트 세트 성능 (원래 스케일):\")\n",
    "for i, param_name in enumerate(target_cols):\n",
    "    param_pred = all_preds_orig[:, i]\n",
    "    param_true = all_targets_orig[:, i]\n",
    "\n",
    "    mse = mean_squared_error(param_true, param_pred)\n",
    "    r2 = r2_score(param_true, param_pred)\n",
    "    metrics[param_name] = {'MSE': mse, 'R2': r2}\n",
    "    print(f\"  {param_name}:\")\n",
    "    print(f\"    MSE: {mse:.4f}\")\n",
    "    print(f\"    R2 Score: {r2:.4f}\")\n",
    "\n",
    "# Visualize Predictions vs. Actuals\n",
    "print(\"\\n예측값 vs. 실제값 그래프 생성 중...\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "# 폰트 및 전역 스타일 설정\n",
    "mpl.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 13,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"lines.linewidth\": 1.8,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"figure.dpi\": 300,  # 고해상도\n",
    "})\n",
    "\n",
    "# 색상 팔레트 선택 (Color Universal Design safe)\n",
    "import seaborn as sns\n",
    "palette = sns.color_palette(\"colorblind\")  # or \"muted\", \"Set2\", \"deep\"\n",
    "\n",
    "# 2x2 그리드로 변경\n",
    "fig, axes = plt.subplots(2, 2, figsize=(11, 10), facecolor='none')  # 배경 투명하게 설정\n",
    "axes = axes.flatten()  # 2D 배열을 1D로 변환하여 쉽게 접근\n",
    "\n",
    "for i, param_name in enumerate(target_cols):\n",
    "    param_pred = all_preds_orig[:, i]\n",
    "    param_true = all_targets_orig[:, i]\n",
    "    \n",
    "    # b와 c 파라미터에 대해 절대값 사용\n",
    "    if param_name == 'b' or param_name == 'c':\n",
    "        param_pred = np.abs(param_pred)\n",
    "        param_true = np.abs(param_true)\n",
    "    \n",
    "    # 오차 허용 밴드 계산 (예측값이 실제값의 ±50% 또는 ±100% 범위 내에 있는지)\n",
    "    ratio = param_pred / param_true\n",
    "    inside_1p5 = np.logical_and(ratio >= 1/1.5, ratio <= 1.5).mean() * 100  # ±50% 오차 범위 내 비율\n",
    "    inside_2x = np.logical_and(ratio >= 0.5, ratio <= 2.0).mean() * 100     # ±100% 오차 범위 내 비율\n",
    "\n",
    "    min_val = min(param_pred.min(), param_true.min()) * 0.9\n",
    "    max_val = max(param_pred.max(), param_true.max()) * 1.1\n",
    "    \n",
    "    # 0이나 음수 값이 있을 경우 로그 스케일에 적합한 최소값 설정\n",
    "    if min_val <= 0:\n",
    "        min_val = 1e-6  # 작은 양수값으로 설정\n",
    "    \n",
    "    line_vals = np.linspace(min_val, max_val, 100)\n",
    "\n",
    "    # 산점도\n",
    "    axes[i].scatter(param_true, param_pred, alpha=0.6, s=40, color=palette[0], edgecolor='k', linewidth=0.3, label=f'$R^2$ = {metrics[param_name][\"R2\"]:.3f}')\n",
    "\n",
    "    # 기준선 (y=x) 및 오차 허용 밴드\n",
    "    axes[i].plot(line_vals, line_vals, 'r-', linewidth=1.5, label='Ideal (y=x)')\n",
    "    \n",
    "    # 1.5x error bands (±50% error)\n",
    "    axes[i].plot(line_vals, line_vals * 1.5, 'g--', linewidth=1.2, label='±50%')\n",
    "    axes[i].plot(line_vals, line_vals / 1.5, 'g--', linewidth=1.2)\n",
    "    \n",
    "    # 2x error bands (±100% error)\n",
    "    axes[i].plot(line_vals, line_vals * 2.0, 'b:', linewidth=1.2, label='±100%')\n",
    "    axes[i].plot(line_vals, line_vals * 0.5, 'b:', linewidth=1.2)\n",
    "\n",
    "    # 로그 스케일 설정 (multiplicative factor가 additive offset으로 보임)\n",
    "    axes[i].set_xscale('log')\n",
    "    axes[i].set_yscale('log')\n",
    "\n",
    "    # Labels and title\n",
    "    if param_name == 'b' or param_name == 'c':\n",
    "        axes[i].set_xlabel(f'Actual |{param_name}|')\n",
    "        axes[i].set_ylabel(f'Predicted |{param_name}|')\n",
    "        axes[i].set_title(f'|{param_name}|\\nWithin ±50% band: {inside_1p5:.1f}%, \\nWithin ±100% band: {inside_2x:.1f}%')\n",
    "    elif param_name == 'spf_MPa':\n",
    "        axes[i].set_xlabel(f'Actual $\\\\sigma\\'_f$')\n",
    "        axes[i].set_ylabel(f'Predicted $\\\\sigma\\'_f$')\n",
    "        axes[i].set_title(f'$\\\\sigma\\'_f$\\nWithin ±50% band: {inside_1p5:.1f}%, \\nWithin ±100% band: {inside_2x:.1f}%')\n",
    "    elif param_name == 'epf':\n",
    "        axes[i].set_xlabel(f'Actual $\\\\varepsilon\\'_f$')\n",
    "        axes[i].set_ylabel(f'Predicted $\\\\varepsilon\\'_f$')\n",
    "        axes[i].set_title(f'$\\\\varepsilon\\'_f$\\nWithin ±50% band: {inside_1p5:.1f}%, \\nWithin ±100% band: {inside_2x:.1f}%')\n",
    "    else:\n",
    "        axes[i].set_xlabel(f'Actual {param_name}')\n",
    "        axes[i].set_ylabel(f'Predicted {param_name}')\n",
    "        axes[i].set_title(f'{param_name}\\nWithin ±50% band: {inside_1p5:.1f}%, \\nWithin ±100% band: {inside_2x:.1f}%')\n",
    "    \n",
    "    axes[i].set_xlim(min_val, max_val)\n",
    "    axes[i].set_ylim(min_val, max_val)\n",
    "    axes[i].legend(frameon=True, loc='upper left')\n",
    "    axes[i].grid(True, linestyle='--', alpha=0.4)\n",
    "    axes[i].set_facecolor('none')  # 각 서브플롯 배경 투명하게 설정\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"FatifueParameters_prediction_actual.pdf\", bbox_inches='tight', transparent=True)  # 투명 배경으로 PDF 저장\n",
    "\n",
    "# 두 번째 그래프 - 지정된 색상(R82 G151 B106)으로 저장\n",
    "custom_color = (82/255, 151/255, 106/255)  # RGB 값을 0-1 범위로 변환\n",
    "\n",
    "# 새로운 그림 생성\n",
    "fig2, axes2 = plt.subplots(2, 2, figsize=(11, 10), facecolor='none')\n",
    "axes2 = axes2.flatten()\n",
    "\n",
    "for i, param_name in enumerate(target_cols):\n",
    "    param_pred = all_preds_orig[:, i]\n",
    "    param_true = all_targets_orig[:, i]\n",
    "    \n",
    "    if param_name == 'b' or param_name == 'c':\n",
    "        param_pred = np.abs(param_pred)\n",
    "        param_true = np.abs(param_true)\n",
    "    \n",
    "    ratio = param_pred / param_true\n",
    "    inside_1p5 = np.logical_and(ratio >= 1/1.5, ratio <= 1.5).mean() * 100\n",
    "    inside_2x = np.logical_and(ratio >= 0.5, ratio <= 2.0).mean() * 100\n",
    "\n",
    "    min_val = min(param_pred.min(), param_true.min()) * 0.9\n",
    "    max_val = max(param_pred.max(), param_true.max()) * 1.1\n",
    "    \n",
    "    if min_val <= 0:\n",
    "        min_val = 1e-6\n",
    "    \n",
    "    line_vals = np.linspace(min_val, max_val, 100)\n",
    "\n",
    "    # 산점도 추가 - 커스텀 색상 그래프에도 산점도 추가\n",
    "    axes2[i].scatter(param_true, param_pred, alpha=0.6, s=40, color=custom_color, edgecolor='k', linewidth=0.3) #label=f'$R^2$ = {metrics[param_name][\"R2\"]:.3f}'\n",
    "\n",
    "    # 모든 선을 지정된 색상으로 설정\n",
    "    axes2[i].plot(line_vals, line_vals, '-', color=custom_color, linewidth=1.5, label='Ideal (y=x)')\n",
    "    axes2[i].plot(line_vals, line_vals * 1.5, '--', color=custom_color, linewidth=1.2, label='±50%')\n",
    "    axes2[i].plot(line_vals, line_vals / 1.5, '--', color=custom_color, linewidth=1.2)\n",
    "    axes2[i].plot(line_vals, line_vals * 2.0, ':', color=custom_color, linewidth=1.2, label='±100%')\n",
    "    axes2[i].plot(line_vals, line_vals * 0.5, ':', color=custom_color, linewidth=1.2)\n",
    "\n",
    "    axes2[i].set_xscale('log')\n",
    "    axes2[i].set_yscale('log')\n",
    "\n",
    "    if param_name == 'b' or param_name == 'c':\n",
    "        #axes2[i].set_xlabel(f'Actual |{param_name}|', color=custom_color)\n",
    "        #axes2[i].set_ylabel(f'Predicted |{param_name}|', color=custom_color)\n",
    "        axes2[i].set_title(f'|{param_name}|\\nWithin ±50% band: {inside_1p5:.1f}%, \\nWithin ±100% band: {inside_2x:.1f}%', color=custom_color)\n",
    "    elif param_name == 'spf_MPa':\n",
    "        #axes2[i].set_xlabel(f'Actual $\\\\sigma\\'_f$', color=custom_color)\n",
    "        #axes2[i].set_ylabel(f'Predicted $\\\\sigma\\'_f$', color=custom_color)\n",
    "        axes2[i].set_title(f'$\\\\sigma\\'_f$\\nWithin ±50% band: {inside_1p5:.1f}%, \\nWithin ±100% band: {inside_2x:.1f}%', color=custom_color)\n",
    "    elif param_name == 'epf':\n",
    "        #axes2[i].set_xlabel(f'Actual $\\\\varepsilon\\'_f$', color=custom_color)\n",
    "        #axes2[i].set_ylabel(f'Predicted $\\\\varepsilon\\'_f$', color=custom_color)\n",
    "        axes2[i].set_title(f'$\\\\varepsilon\\'_f$\\nWithin ±50% band: {inside_1p5:.1f}%, \\nWithin ±100% band: {inside_2x:.1f}%', color=custom_color)\n",
    "    else:\n",
    "        #axes2[i].set_xlabel(f'Actual {param_name}', color=custom_color)\n",
    "        #axes2[i].set_ylabel(f'Predicted {param_name}', color=custom_color)\n",
    "        axes2[i].set_title(f'{param_name}\\nWithin ±50% band: {inside_1p5:.1f}%, \\nWithin ±100% band: {inside_2x:.1f}%', color=custom_color)\n",
    "    \n",
    "    axes2[i].set_xlim(min_val, max_val)\n",
    "    axes2[i].set_ylim(min_val, max_val)\n",
    "    axes2[i].legend(frameon=True, loc='upper left', labelcolor=custom_color)\n",
    "    axes2[i].grid(True, linestyle='--', alpha=0.4, color=custom_color)\n",
    "    axes2[i].set_facecolor('none')\n",
    "    \n",
    "    # 테두리 색상을 custom_color로 변경\n",
    "    for spine in axes2[i].spines.values():\n",
    "        spine.set_color(custom_color)\n",
    "        spine.set_linewidth(1.2)\n",
    "    \n",
    "    # 축 눈금 및 레이블 색상 변경\n",
    "    axes2[i].tick_params(axis='both', colors=custom_color)\n",
    "    \n",
    "    # 로그 스케일 눈금 값들의 색상 변경 - 모든 그래프에 적용\n",
    "    for label in axes2[i].get_xticklabels():\n",
    "        label.set_color(custom_color)\n",
    "    for label in axes2[i].get_yticklabels():\n",
    "        label.set_color(custom_color)\n",
    "    \n",
    "    # 범례 텍스트 색상 변경\n",
    "    legend = axes2[i].get_legend()\n",
    "    for text in legend.get_texts():\n",
    "        text.set_color(custom_color)\n",
    "\n",
    "    # 로그 스케일 눈금 색상 변경 추가 확인 (sigma'f와 c 그래프용)\n",
    "    if param_name == 'spf_MPa' or param_name == 'c':\n",
    "        # 메이저 및 마이너 눈금 모두 색상 변경\n",
    "        axes2[i].tick_params(axis='x', which='both', colors=custom_color)\n",
    "        axes2[i].tick_params(axis='y', which='both', colors=custom_color)\n",
    "        \n",
    "        # 모든 눈금 레이블 색상 명시적 설정\n",
    "        for label in axes2[i].xaxis.get_ticklabels(which='both'):\n",
    "            label.set_color(custom_color)\n",
    "        for label in axes2[i].yaxis.get_ticklabels(which='both'):\n",
    "            label.set_color(custom_color)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"FatifueParameters_prediction_actual_green.pdf\", bbox_inches='tight', transparent=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: E-N 및 Gamma-N 곡선 생성 함수 (수정됨)\n",
    "\n",
    "# df 변수 확인 없이 바로 함수 정의\n",
    "def predict_fatigue_curves(E_val, YS_val, TS_val, HB_val, model, scaler_X, scaler_y, device, mode='both', nu=0.3):\n",
    "    \"\"\"\n",
    "    인장 파라미터를 예측하고 E-N 및 Gamma-N 곡선 데이터를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        E_val (float): 탄성 계수 (MPa)\n",
    "        YS_val (float): 항복 강도 (MPa)\n",
    "        TS_val (float): 인장 강도 (MPa)\n",
    "        HB_val (float): 브리넬 경도 (처리된 값)\n",
    "        model (torch.nn.Module): 학습된 FatiguePINN 모델\n",
    "        scaler_X (StandardScaler): 입력 특성에 대한 스케일러\n",
    "        scaler_y (StandardScaler): 출력 파라미터에 대한 스케일러\n",
    "        device (str): 장치 ('cpu', 'cuda', 'mps')\n",
    "        mode (str, optional): 반환 모드 ('tensile', 'shear', 'both'). 기본값은 'both'.\n",
    "        nu (float, optional): 포아송 비. 기본값은 0.3.\n",
    "\n",
    "    Returns:\n",
    "        mode에 따라 다른 값을 반환합니다:\n",
    "        - 'tensile': (tensile_params, reversals, strain_amplitude_en, elastic_strain, plastic_strain)\n",
    "        - 'shear': (shear_params, reversals, strain_amplitude_gn, elastic_shear_strain, plastic_shear_strain)\n",
    "        - 'both': (tensile_params, reversals, strain_amplitude_en, elastic_strain, plastic_strain,\n",
    "                  shear_params, reversals, strain_amplitude_gn, elastic_shear_strain, plastic_shear_strain)\n",
    "    \"\"\"\n",
    "    model.eval() # 모델을 평가 모드로 설정\n",
    "\n",
    "    # --- 1. 인장 파라미터 예측 ---\n",
    "    input_features = np.array([[E_val, YS_val, TS_val, HB_val]])\n",
    "    input_scaled = scaler_X.transform(input_features)\n",
    "    input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_tensile_params_scaled = model(input_tensor)\n",
    "    predicted_tensile_params_orig = scaler_y.inverse_transform(predicted_tensile_params_scaled.cpu().numpy())[0]\n",
    "\n",
    "    tensile_target_cols_local = ['spf_MPa', 'b', 'epf', 'c'] # 여기서 다시 정의 (함수 범위)\n",
    "    tensile_params = {name: val for name, val in zip(tensile_target_cols_local, predicted_tensile_params_orig)}\n",
    "    spf_prime = tensile_params['spf_MPa']\n",
    "    b = tensile_params['b']\n",
    "    epf_prime = tensile_params['epf']\n",
    "    c = tensile_params['c']\n",
    "\n",
    "    reversals = np.logspace(1, 7, num=100) # 공통 사용\n",
    "\n",
    "    # --- 2. 인장 E-N 곡선 계산 ---\n",
    "    E_val_safe = max(E_val, 1e-6) # 0 방지\n",
    "    elastic_strain = (spf_prime / E_val_safe) * (reversals ** b)\n",
    "    plastic_strain = epf_prime * (reversals ** c)\n",
    "    strain_amplitude_en = elastic_strain + plastic_strain\n",
    "\n",
    "    # 인장 모드만 요청된 경우 여기서 반환\n",
    "    if mode == 'tensile':\n",
    "        return tensile_params, reversals, strain_amplitude_en, elastic_strain, plastic_strain\n",
    "\n",
    "    # --- 3. 전단 파라미터 변환 ---\n",
    "    shear_params = {}\n",
    "    conversion_method = \"Unknown\"\n",
    "\n",
    "    # von Mises 변환\n",
    "    tau_vm = spf_prime / np.sqrt(3)\n",
    "    gamma_vm = np.sqrt(3) * epf_prime\n",
    "\n",
    "    # Max Principal 변환\n",
    "    tau_mp = spf_prime / (1 + nu)\n",
    "    gamma_mp = 2 * epf_prime\n",
    "\n",
    "    if TS_val <= 1100:\n",
    "        # 순수 von Mises\n",
    "        tauf_prime = tau_vm\n",
    "        gammaf_prime = gamma_vm\n",
    "        b0 = b\n",
    "        c0 = c\n",
    "        conversion_method = \"von Mises\"\n",
    "    elif TS_val >= 1696:\n",
    "        # 순수 Max Principal\n",
    "        tauf_prime = tau_mp\n",
    "        gammaf_prime = gamma_mp\n",
    "        b0 = b\n",
    "        c0 = c\n",
    "        conversion_method = \"Max Principal\"\n",
    "    else:\n",
    "        # 1100<TS<1696인 mid-range 구간: von Mises→Max-Principle 선형 인터폴\n",
    "        α = (TS_val - 1100) / (1696 - 1100)  # 0.0~1.0\n",
    "        tauf_prime = (1-α)*tau_vm + α*tau_mp\n",
    "        gammaf_prime = (1-α)*gamma_vm + α*gamma_mp\n",
    "        b0 = b\n",
    "        c0 = c\n",
    "        conversion_method = f\"Interp VM→MP (α={α:.2f})\"\n",
    "\n",
    "    # 변환된 파라미터 저장\n",
    "    shear_params['tauf_MPa'] = tauf_prime\n",
    "    shear_params['gammaf'] = gammaf_prime\n",
    "    shear_params['b0'] = b0\n",
    "    shear_params['c0'] = c0\n",
    "    shear_params['conversion_method'] = conversion_method\n",
    "\n",
    "    # --- 4. 전단 탄성 계수 G 계산 ---\n",
    "    G_val = E_val_safe / (2 * (1 + nu))\n",
    "\n",
    "    # --- 5. 전단 Gamma-N 곡선 계산 ---\n",
    "    elastic_shear_strain = (tauf_prime / G_val) * (reversals ** b0)\n",
    "    plastic_shear_strain = gammaf_prime * (reversals ** c0)\n",
    "    strain_amplitude_gn = elastic_shear_strain + plastic_shear_strain\n",
    "\n",
    "    # 전단 모드만 요청된 경우\n",
    "    if mode == 'shear':\n",
    "        return shear_params, reversals, strain_amplitude_gn, elastic_shear_strain, plastic_shear_strain\n",
    "    \n",
    "    # 기본값: 모두 반환\n",
    "    return tensile_params, reversals, strain_amplitude_en, elastic_strain, plastic_strain, shear_params, reversals, strain_amplitude_gn, elastic_shear_strain, plastic_shear_strain\n",
    "\n",
    "# --- 함수 테스트 ---\n",
    "print(\"\\n--- Testing E-N and Gamma-N Curve Generation ---\")\n",
    "try:\n",
    "    # 테스트셋의 첫 번째 샘플 사용\n",
    "    example_input_X = X_test[0] # 원본 스케일 특징\n",
    "    example_E = E_test[0]       # 원본 스케일 E\n",
    "    example_TS = example_input_X[2] # 원본 스케일 TS\n",
    "    example_HB = example_input_X[-1]  # 처리된 HB (마지막 특징)\n",
    "\n",
    "    print(f\"Example Input:\")\n",
    "    print(f\"  E: {example_E:.0f} MPa\")\n",
    "    print(f\"  YS: {example_input_X[1]:.0f} MPa\")\n",
    "    print(f\"  TS: {example_TS:.0f} MPa\")\n",
    "    print(f\"  HB: {example_HB:.1f}\")\n",
    "\n",
    "    tensile_p, rev_en, strain_en, elastic_strain, plastic_strain, shear_p, rev_gn, strain_gn, elastic_shear_strain, plastic_shear_strain = predict_fatigue_curves(\n",
    "        example_E, example_input_X[1], example_TS, example_HB,\n",
    "        model, scaler_X, scaler_y, device, mode='both', nu=0.3\n",
    "    )\n",
    "\n",
    "    print(\"\\n인장 파라미터 예측 결과:\")\n",
    "    for name, val in tensile_p.items(): print(f\"  {name}: {val:.4f}\")\n",
    "\n",
    "    print(\"\\n전단 파라미터 예측 결과:\")\n",
    "    for name, val in shear_p.items(): print(f\"  {name}: {val if isinstance(val, str) else f'{val:.4f}'}\")\n",
    "\n",
    "    # 한글 폰트 설정\n",
    "    import matplotlib.font_manager as fm\n",
    "    # 시스템에 설치된 폰트 목록 확인 (선택사항)\n",
    "    # print([f.name for f in fm.fontManager.ttflist])\n",
    "    \n",
    "    # 한글 폰트 설정 (시스템에 설치된 한글 폰트 중 하나 선택)\n",
    "    plt.rcParams['font.family'] = 'AppleGothic'  # macOS\n",
    "    # plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows\n",
    "    # plt.rcParams['font.family'] = 'NanumGothic'  # 나눔고딕 설치된 경우\n",
    "    plt.rcParams['axes.unicode_minus'] = False  # 마이너스 기호 깨짐 방지\n",
    "\n",
    "    # E-N 곡선 플롯 (구성 요소 포함)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.loglog(rev_en, strain_en, '-', label='총 변형률 (epsilon_a)')\n",
    "    plt.loglog(rev_en, elastic_strain, '--', label='탄성 변형률', alpha=0.7)\n",
    "    plt.loglog(rev_en, plastic_strain, ':', label='소성 변형률', alpha=0.7)\n",
    "    plt.xlabel('파괴까지의 반복 횟수 (2Nf)')\n",
    "    plt.ylabel('변형률 진폭 (epsilon_a)')\n",
    "    plt.title('예측된 E-N 곡선 (구성 요소 포함)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.ylim(bottom=1e-4)\n",
    "    plt.show()\n",
    "\n",
    "    # Gamma-N 곡선 플롯 (구성 요소 포함)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.loglog(rev_gn, strain_gn, '-', label='총 전단 변형률 (gamma_a)')\n",
    "    plt.loglog(rev_gn, elastic_shear_strain, '--', label='전단 탄성 변형률', alpha=0.7)\n",
    "    plt.loglog(rev_gn, plastic_shear_strain, ':', label='전단 소성 변형률', alpha=0.7)\n",
    "    plt.xlabel('파괴까지의 반복 횟수 (2Nf)')\n",
    "    plt.ylabel('전단 변형률 진폭 (gamma_a)')\n",
    "    plt.title(f'예측된 Gamma-N 곡선 (구성 요소 포함, UTS={example_TS:.0f}, 방법: {shear_p[\"conversion_method\"]})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.ylim(bottom=1e-4)\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "     print(\"오류: 테스트 데이터(X_test, E_test) 또는 모델/스케일러를 사용할 수 없습니다. 이전 셀을 실행하세요.\")\n",
    "except IndexError:\n",
    "     print(\"오류: 테스트 데이터가 비어 있거나 형태가 올바르지 않습니다.\")\n",
    "except Exception as e:\n",
    "     print(f\"테스트 중 오류가 발생했습니다: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
